---
title: 'Differential Privacy as Hypothesis Testing'
date: 2022-12-01
permalink: /posts/2022/8/dp-ht/
tags:
  - Differential Privacy
  - Hypothesis Testing
---

Connecting the dots between Differential Privacy, Hypothesis Testing and (ideal) adversaries.


------

# 1. Differential Privacy as Hypothesis Testing


 [Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) (DP) is the golden standard used both in academia and industry to reason about how private a given algorithm is,  but its adoption rate is rather small due to unclear guarantees it provides you, esoteric epsilon values and not so straightforward distribution distances. Even more, if you decide to use relaxations of DP, like [Approximate DP](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf), [Renyi-DP](https://arxiv.org/pdf/1702.07476.pdf), [f-DP](https://arxiv.org/pdf/1905.02383.pdf) and so on, you might feel intimidated and a bit lost. In this blog post, I would try to shed some light on what DP is trying to achieve from the lens of an adversary that tries to break our privacy and how DP **guarantees** to ruin the chances of the attacker to succeed.

## Pure Differential Privacy:

The formulation of pure differential privacy states that an algorithm M provides $\epsilon$-DP privacy if for two  databases that differ in one element $D_1$  and $D_2$ and an output space O, the following property holds: 

$$
                    P[M(D) \in O] < e^\epsilon * P[M(D') \in O]
$$

A nice way of understanding DP is that if algorithm A does not release any kind of information about the existence of a given sample in the database, then the privacy of that sample is protected.  A quote that describes best DP is made directly by Dwork and Roth in their seminal book: 

> *Differential privacy describes a promise, made by a data holder, or curator, to a data subject: “You will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available.”*
> 

![spiderman_meme.jpg](https://github.com/tudorcebere/profile/blob/master/files/blog_resources/dp_ht/spiderman_meme.jpg?raw=true)

This sounds neat! Before talking about algorithms that try to achieve $\epsilon$-DP and keep their promise to the data subject, let's discuss about adversaries. To develop a strong defence, we need to understand whom we are protecting against, right **(or maybe not!)** ?

## Differential Privacy Adversaries

Differential privacy became the standard in privacy because it protects the database used as input for a given algorithm $M$ without modelling the power of the adversary, namely, the adversary can have any kind of auxiliary information or compute power, we have a statistical indistinguishability guarantee that shields our data privacy. A bit more formally, for a given algorithm M, the distinguishing game played between two parties, $C$ - the challenger that owns the data whose privacy we should protect and $A$, the adversary that wants to recover a bit of information is:


1. **Setup**: Challenger outputs public knowledge: $M, D_1, D_2$
2. **Challenge**: Challenger pick random bit $b \overset{\$}{\leftarrow} \{ 1, 2 \}$ and sends to the adversary $O \leftarrow M(D_b)$  
3. **Distinguish**: Adversary $A$ runs a distinguishing algorithm $b’ \leftarrow$  Distinguish($O$, $D_1$, $D_2$, $M$) and outputs it's guess b'
4. **Result** Adversary wins if $b == b’$

Given the above definition, we observe that differential privacy abstracts away from how $A$ tries to distinguish and from any auxiliary input that it may know, and from the content of $D_1$ and $D_2$ making the definition sound for cryptographic purposes! We can imagine now that if we write our algorithm $M$ in such a way that it respects differential privacy, we abstract away from data itself or adversaries. 

## Differential Privacy as Hypothesis Testing

Hypothesis Testing is the defacto way of understanding how plausible is a given statement using the data that you have provided.  The hypothesis that we want to prove is that an attacker is not effective against our  $\epsilon$-DP algorithm and we can bound his performance using this $\epsilon$. More formally, for any adversary $A$ that tries to distinguish from the output of your algorithm M if the underlying used database was either $D_1$ or $D_2$ the possible outcomes are:

- $H_0$  (null) - $O$ was generated by running $M(D_1)$
- $H_1$  (alternative) - $O$ was generated by running $M(D_2)$
- Type I Error - $A$  outputs $D_1$ and the underlying dataset was $D_2$, we will denote this probability as FNR
- Type II Error - $A$ outputs D2 and the underlying dataset was $D_1$, we will denote this probability as FPR

Before going any further, please take a moment to take a look at the confusion matrix definition and how

FNR and FPR are defined. I feel like this is one of the most confusing parts of statistics, don’t worry if you mix them up!

What does it mean for an attacker to be successful? It should achieve both small FNR and FPR, meaning that it can properly distinguish correctly.  The seminal works of [Kairouz](https://arxiv.org/pdf/1311.0776.pdf) and [Zhang](https://arxiv.org/abs/0811.2501) gave us a strong property on the attacker performance - If an algorithm is $\epsilon$-DP, then the error rates respect the following bound:

$$
FNR + e^\epsilon * FPR > 1 
$$

$$

FPR + e^\epsilon * FNR > 1
$$

This means that an attacker can have some regions, whom we will call trust regions, in which he is bounded to act, determined only by $\epsilon$. These trust regions directly affect the [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve and [AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=AUC%20represents%20the%20probability%20that,has%20an%20AUC%20of%201.0.) score of the success of the attacker in a way that we will analyse further in this blog post.

A small reminder of the confusion matrix and its relationship with hypothesis testing

| | True Positive (TP) | False Positive (FP)|
| True Negative (TN)|  TPR = $\frac{TP}{TP + FN}$| FNR = \frac{FN}{TP + FN}|
| False Negative(FN)|| FPR = $\frac{FP}{TN + FP}$| TNR = \frac{TN}{TN + FP}|


Epsilon lower bounds mapping for the possible value ranges of FPR and FNR in $\epsilon$-DP.

![fnr_fpr_epsilon.png](https://github.com/tudorcebere/profile/blob/master/files/blog_resources/dp_ht/fnr_fpr_epsilon.png?raw=true)

Let’s get a basic differentially private mechanism, instantiate an attacker against it and see how it behaves! This will give us a neat understanding of why we care about Differential Privacy.

## Analyzing the randomized mechanism for counting

 A counting query is an algorithm that takes database $D$ and outputs the fraction of elements that satisfy a predicate $q$.  The randomized mechanism is a $\epsilon$-DP algorithm used for counting queries defined as:

$$
M_{count}(D) = \overset{|D|}{ \underset{i}{\sum}} q(D[i])
$$

$$
q(x) = \begin{cases} \lnot q(x), \text{with probability } (1 - p)/2 \\  q(x), \text{with probability } (1 + p)/2 \end{cases}
$$

This mechanism is  $ln\frac{1 - p}{1 + p}$-DP. Let's implement this in plain Python for a cardinality algorithm. Namely, we would like to estimate the length of the database in a differentially private way:


```python
import random

def private_count(D: list, p: float) -> int:
  count_result = 0
  for i in range(len(D)):
    sample = random.random()
    if sample < (1 + p)/2:
      count_result += 1
  return count_result
```

Let’s create a simple attacker against this query type and see how the ROC curve and AUC score evolves with the epsilon. The distinguishing function of our attacker will be to always to pick the dataset with the closest length as the given output of the mechanism.  Remember that $\epsilon$-DP should work against any underlying data or attacker, so we should try as hard as possible to create a degenerate edge case! 

```python
def attack(D1: list, D2: list, out: int) -> int:
   diff_1 = abs(len(D1) - out)
   diff_2 = abs(len(D2) - out)
   return 1 if diff_1 < diff_2 else 2
```

```python
D1 = [1] # this datasets length is 1
D2 = [1, 2] # this datasets length is 2
number_of_experiments = 10000
p = 0.2

for i in range(number_of_experiments):
   O, b = private_count(D1, D2, p)
   b_prime = attack(D1, D2, O)
   # ... store results and plot ROC curve/compute AUC ...
   # (the full code is available at the end of the blogpost
```

![roc_auc_attacker.png](https://github.com/tudorcebere/profile/blob/master/files/blog_resources/dp_ht/roc_auc_attacker.png?raw=true)

The ideal degenerate case is when there are a few samples to distinguish from! We will evaluate the effectiveness of this attack on a fixed very small length, in which the errors are big and the distinguishing power is significant. Our attacker was crafted just for the sake of evaluating how the performance of the adversary degrades based on the epsilon. Note that this is probably not the strongest attacker, but it is good enough to show the performance on an edge case! There might be way stronger attackers that we don’t know how to create, but $\epsilon$-DP has to bound the performance of all of them.

An impressive property is that the attacker bounding properties holds for all algorithms that satisfy $\epsilon$-DP, for example, the [Laplacian mechanism](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) or the [Exponential mechanism](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf). Now, let’s see analyse how one of the most well-known relaxations of $\epsilon$-DP behaves from an attacker's point of view!

## Approximate DP

Approximate DP is a relaxation to the pure-DP formulation in which we use a negligible term $\delta$ to describe a catastrophic failure, for example, leaking a part of the private data. This type of relaxation is common in cryptography to achieve practicality, one of the most common ways of thinking about this is that an $(\epsilon, \delta)$-DP algorithm is $\epsilon$-DP with a probability of $1 - \delta$.  More formally, an algorithm M provides $(\epsilon, \delta)$-DP privacy if, for two  databases that differ in one element $D_1$  and $D_2$ and an output space O, the following property holds: 

                    $P[M(D_1) \in O] < e^\epsilon * P[M(D_2) \in O] + \delta$ 

We can observe how this formulation makes our analysis of the performance of an attacker a bit more tedious, as we have a new parameter to take into account when trying to understand the trust region in which an attacker is allowed to perform. Let's plot them for fixed values of  $\delta$!

Observe how the region between the blue dotted line and the 0 epsilon marker line evolves. When $\delta$=0, they overlap, we are in the same case as $\epsilon$-DP. As delta increases, there is a region between the $\epsilon=0$ line and the blue dotted line that is considered perfectly private, even if it’s not! Pure DP showed us how in that region we can have catastrophic attackers who can break privacy. 

 This gives relaxation to our epsilon needs (and, but we decide to ignore a part of the behaviour of the attacker. This is why epsilon has to be cryptographically small,   it is a useful tool for tail bounding very improbable scenarios, but we don’t want to allow too many of them!

One of the things that didn’t click in my head when I was first reading the definition of Approximate-DP is the connection between $\delta$ and the probability that information about the dataset could be leaked. The Hypothesis Testing formulation against a worst-case adversary clearly presents this! It is easy to observe how behaviour like FPR = 0.85 and FNR=0 is marked as perfectly private in $(0, 0.15)$-DP, even if it’s not.

Another intuitive way that will be very handy when we try to understand the Optimal Composition Theorem for Approximate-DP under k-folding is to plot the trust region of an attacker at a given $(\epsilon, \delta)$. Let’s take a look at such surface:

 ![Privacy Profiles.png](https://github.com/tudorcebere/profile/blob/master/files/blog_resources/dp_ht/download_(2).png?raw=true)

## End Note

I think this is enough information for one sitting, we have a good basis for understanding how adversaries can interact with challengers that use a mechanism that is either $\epsilon$-DP or $(\epsilon, \delta)$-DP.  The challenger doesn’t care about either the data or the attacker's knowledge, our defence mechanism is a worst-case scenario one, good luck on breaking that! There are a lot of questions that are left open that I will try to answer in future blog posts, namely:
 
- Using this idea of trust regions, derive together the equations for optimal composition!
- Can we use this information to actually test DP implementations? If we find an attacker that violates the guarantees given by DP, most likely the implementation is incorrect and it’s worthwhile understanding how we can prove that!
- Can we [evaluate](https://arxiv.org/pdf/2006.07709.pdf) the epsilon that an algorithm provides against an [instantiable](https://arxiv.org/abs/2101.04535) adversary?

## [Code Link](https://colab.research.google.com/drive/1rk-JZFZ6NCf50ULEwN7lbPXENCpA3iVP)

![jack_nicholson.jpg](https://github.com/tudorcebere/profile/blob/master/files/blog_resources/dp_ht/jack_nicholson.jpg?raw=true)